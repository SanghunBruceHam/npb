name: NPB Crawler

on:
  # Îß§Ïùº JST Ïò§ÌõÑ 4:30 ~ 11:30ÍπåÏßÄ 30Î∂ÑÎßàÎã§ Ïã§Ìñâ
  # JST 16:30-23:30 = UTC 07:30-14:30
  schedule:
    - cron: '30 7-14 * * *'  # 30Î∂ÑÎßàÎã§ (JST 16:30-23:30)
  
  # ÏàòÎèô Ïã§Ìñâ
  workflow_dispatch:
    inputs:
      days_back:
        description: 'Number of days to crawl back'
        required: false
        default: '3'
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client
    
    - name: Install Python dependencies
      run: |
        cd crawler
        pip install requests beautifulsoup4 psycopg2-binary python-dotenv pytz schedule
    
    - name: Run NPB Crawler
      env:
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
      run: |
        cd crawler
        DAYS_BACK="${{ github.event.inputs.days_back || '3' }}"
        echo "üöÄ Starting NPB crawl (last $DAYS_BACK days)"
        python main.py $DAYS_BACK
    
    - name: Database Health Check
      if: github.event_name == 'workflow_dispatch'
      env:
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
      run: |
        cd crawler
        python -c "
        import psycopg2
        import os
        from datetime import datetime
        
        db_config = {
            'host': os.getenv('DB_HOST'),
            'port': os.getenv('DB_PORT', '5432'),
            'database': os.getenv('DB_NAME'),
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD', '')
        }
        
        try:
            conn = psycopg2.connect(**db_config)
            cur = conn.cursor()
            
            # Ï¥ù Í≤ΩÍ∏∞Ïàò ÌôïÏù∏
            cur.execute('SELECT COUNT(*) FROM games')
            total_games = cur.fetchone()[0]
            
            # ÏµúÍ∑º 7Ïùº Í≤ΩÍ∏∞Ïàò
            cur.execute(\"\"\"
                SELECT COUNT(*) FROM games 
                WHERE game_date >= CURRENT_DATE - INTERVAL '7 days'
            \"\"\")
            recent_games = cur.fetchone()[0]
            
            # ÌåÄÎ≥Ñ Í≤ΩÍ∏∞Ïàò
            cur.execute(\"\"\"
                SELECT COUNT(DISTINCT team_id) FROM standings 
                WHERE season_year = 2025
            \"\"\")
            teams_count = cur.fetchone()[0]
            
            print(f'üìä Database Health Check:')
            print(f'   Total games: {total_games}')
            print(f'   Recent games (7 days): {recent_games}')
            print(f'   Teams in standings: {teams_count}')
            
            cur.close()
            conn.close()
            
            print('‚úÖ Database health check passed')
            
        except Exception as e:
            print(f'‚ùå Database health check failed: {e}')
            exit(1)
        "
    
    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: crawler-logs-${{ github.run_number }}
        path: crawler/logs/
        retention-days: 7