#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NPB Daily Crawler Script
ë§¤ì¼ ìë™ í¬ë¡¤ë§ ë° ë°ì´í„° ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'crawler'))

import subprocess
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
import json
import psycopg2
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
def setup_logging():
    log_dir = Path(__file__).parent.parent / "logs" / "daily_crawler"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = log_dir / f"daily_{datetime.now().strftime('%Y%m%d')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('daily_crawler')

logger = setup_logging()

class DailyCrawler:
    def __init__(self):
        self.crawler_path = Path(__file__).parent.parent / "crawler" / "npb_crawler.py"
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': os.getenv('DB_PORT', '5432'),
            'database': os.getenv('DB_NAME', 'npb_dashboard_dev'),
            'user': os.getenv('DB_USER', 'sanghunbruceham'),
            'password': os.getenv('DB_PASSWORD', '')
        }
        
    def run_crawler(self, days_back=3):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            logger.info(f"Starting crawler for last {days_back} days...")
            
            cmd = [str(self.crawler_path), str(days_back)]
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                logger.info("âœ… Crawler completed successfully")
                logger.info(f"Output: {result.stdout}")
                return True
            else:
                logger.error(f"âŒ Crawler failed: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ Crawler timed out")
            return False
        except Exception as e:
            logger.error(f"âŒ Crawler error: {e}")
            return False
    
    def cleanup_old_data(self, days_to_keep=90):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        try:
            logger.info(f"Cleaning up data older than {days_to_keep} days...")
            
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            with psycopg2.connect(**self.db_config) as conn:
                with conn.cursor() as cur:
                    # ì˜¤ë˜ëœ í¬ë¡¤ë§ ë¡œê·¸ ì‚­ì œ
                    cur.execute(
                        "DELETE FROM crawl_logs WHERE crawl_timestamp < %s",
                        (cutoff_date,)
                    )
                    deleted_logs = cur.rowcount
                    
                    # ë§¤ìš° ì˜¤ë˜ëœ ê²Œì„ ë°ì´í„°ëŠ” ë³´ê´€ (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
                    logger.info(f"âœ… Cleaned up {deleted_logs} old crawl logs")
                    
        except Exception as e:
            logger.error(f"âŒ Cleanup failed: {e}")
    
    def backup_data(self):
        """ë°ì´í„° ë°±ì—…"""
        try:
            logger.info("Creating daily data backup...")
            
            backup_dir = Path(__file__).parent.parent / "data" / "backups"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            today = datetime.now().strftime('%Y%m%d')
            
            with psycopg2.connect(**self.db_config) as conn:
                with conn.cursor() as cur:
                    # ìµœê·¼ ê²Œì„ ë°ì´í„° ë°±ì—…
                    cur.execute("""
                        SELECT json_agg(g.*)
                        FROM games g 
                        WHERE g.game_date >= CURRENT_DATE - INTERVAL '7 days'
                    """)
                    games_data = cur.fetchone()[0]
                    
                    if games_data:
                        backup_file = backup_dir / f"games_backup_{today}.json"
                        with open(backup_file, 'w', encoding='utf-8') as f:
                            json.dump(games_data, f, ensure_ascii=False, indent=2, default=str)
                        
                        logger.info(f"âœ… Data backed up to {backup_file}")
                    
        except Exception as e:
            logger.error(f"âŒ Backup failed: {e}")
    
    def check_data_health(self):
        """ë°ì´í„° ìƒíƒœ ì ê²€"""
        try:
            with psycopg2.connect(**self.db_config) as conn:
                with conn.cursor() as cur:
                    # ì˜¤ëŠ˜ ê²Œì„ ë°ì´í„° í™•ì¸
                    cur.execute("""
                        SELECT COUNT(*) 
                        FROM games 
                        WHERE game_date = CURRENT_DATE
                    """)
                    today_games = cur.fetchone()[0]
                    
                    # ìµœê·¼ 7ì¼ ê²Œì„ ìˆ˜ í™•ì¸
                    cur.execute("""
                        SELECT COUNT(*) 
                        FROM games 
                        WHERE game_date >= CURRENT_DATE - INTERVAL '7 days'
                    """)
                    week_games = cur.fetchone()[0]
                    
                    logger.info(f"ğŸ“Š Health Check - Today: {today_games} games, Week: {week_games} games")
                    
                    # ì´ìƒ ê°ì§€
                    if week_games < 20:  # ì¼ì£¼ì¼ì— 20ê²½ê¸° ë¯¸ë§Œì´ë©´ ì´ìƒ
                        logger.warning("âš ï¸ Low game count detected - check crawler")
                        return False
                    
                    return True
                    
        except Exception as e:
            logger.error(f"âŒ Health check failed: {e}")
            return False
    
    def run_daily_maintenance(self):
        """ì¼ì¼ ì •ê¸° ì‘ì—… ì‹¤í–‰"""
        logger.info("ğŸš€ Starting daily NPB maintenance...")
        
        success = True
        
        # 1. ê²°ê³¼ ê²½ê¸° í¬ë¡¤ë§
        if not self.run_crawler():
            success = False
        
        # ì˜ˆì • ê²½ê¸°ëŠ” results_crawlerì—ì„œ í•¨ê»˜ ì²˜ë¦¬
        
        # 2. DB â†’ JSON ë™ê¸°í™” (ìë™)
        logger.info("ğŸ”„ Syncing DB to JSON...")
        try:
            import subprocess
            sync_cmd = [
                str(Path(__file__).parent / "data_manager.py"),
                "--sync"
            ]
            result = subprocess.run(sync_cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("âœ… DB to JSON sync completed")
            else:
                logger.error(f"âŒ DB to JSON sync failed: {result.stderr}")
                success = False
        except Exception as e:
            logger.error(f"âŒ JSON sync error: {e}")
            success = False
        
        # 3. ë°ì´í„° ìƒíƒœ ì ê²€
        if not self.check_data_health():
            success = False
        
        # 4. ë°ì´í„° ë°±ì—…
        self.backup_data()
        
        # 5. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        self.cleanup_old_data()
        
        if success:
            logger.info("âœ… Daily maintenance completed successfully")
        else:
            logger.error("âŒ Daily maintenance completed with errors")
        
        return success

def main():
    crawler = DailyCrawler()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == '--crawler-only':
            success = crawler.run_crawler()
        elif sys.argv[1] == '--backup-only':
            crawler.backup_data()
            success = True
        elif sys.argv[1] == '--cleanup-only':
            crawler.cleanup_old_data()
            success = True
        else:
            success = crawler.run_daily_maintenance()
    else:
        success = crawler.run_daily_maintenance()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()