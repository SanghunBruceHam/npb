#!/usr/bin/env python3
"""
ì™„ì „ ìƒˆë¡œìš´ NPB Pipeline
ì›¹ í¬ë¡¤ë§ â†’ TXT ì €ì¥ â†’ JavaScript ì²˜ë¦¬ â†’ JSON ì €ì¥
"""

import sys
import os
import subprocess
from pathlib import Path
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent

def setup_logging():
    """New Pipeline ì „ìš© ë¡œê¹… ì„¤ì •"""
    log_dir = project_root / "logs" / "new_pipeline"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = log_dir / f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('new_pipeline')

def run_web_crawler(mode="7", use_legacy=False):
    """ì›¹ í¬ë¡¤ë§ ì‹¤í–‰ (TXT ì§ì ‘ ì €ì¥)
    ê¸°ë³¸: ìµœì†Œ ê²½ë¡œ(min_results_crawler.py) ì‚¬ìš©
    --legacy-crawler ì˜µì…˜ìœ¼ë¡œ ê¸°ì¡´ simple_crawler ì‚¬ìš© ê°€ëŠ¥
    """
    if mode == "full-season":
        logger.info("ğŸ•·ï¸ Starting FULL SEASON web crawling (from March 28)...")
        timeout = 1800  # 30ë¶„ (ì „ì²´ ì‹œì¦Œ)
    else:
        logger.info(f"ğŸ•·ï¸ Starting web crawling for {mode} days...")
        timeout = 300   # 5ë¶„ (ì¼ë°˜)

    try:
        if use_legacy:
            crawler_path = project_root / 'crawler' / 'simple_crawler.py'
        else:
            crawler_path = project_root / 'crawler' / 'min_results_crawler.py'

        if mode == "full-season":
            cmd = ['python3', str(crawler_path), '--full-season']
        else:
            # min crawler supports bare integer argument too
            cmd = ['python3', str(crawler_path), str(mode)]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout
        )

        if result.returncode == 0:
            logger.info("âœ… Web crawling completed successfully")
            if result.stdout:
                logger.info(f"Crawler output: {result.stdout}")
            if result.stderr:
                logger.info(f"Crawler stderr: {result.stderr}")
            return True
        else:
            logger.error(f"âŒ Web crawling failed: {result.stderr}")
            return False

    except Exception as e:
        logger.error(f"âŒ Web crawling error: {e}")
        return False

def convert_txt_to_json():
    """TXT â†’ JavaScript ì²˜ë¦¬ â†’ JSON ì €ì¥"""
    logger.info("ğŸ”„ Converting TXT to JSON via JavaScript...")
    
    try:
        cmd = ['node', str(project_root / 'scripts' / 'simple_txt_to_json.js')]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            logger.info("âœ… TXT to JSON conversion completed")
            logger.info(f"Conversion output: {result.stdout}")
            return True
        else:
            logger.error(f"âŒ TXT to JSON conversion failed: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ TXT to JSON conversion error: {e}")
        return False

def validate_output_files():
    """ì¶œë ¥ íŒŒì¼ë“¤ ê²€ì¦"""
    logger.info("ğŸ” Validating output files...")
    
    required_files = [
        'standings.json',
        'games.json', 
        'teams.json',
        'dashboard.json'
    ]
    
    data_dir = project_root / 'data'
    valid_files = 0
    
    for filename in required_files:
        file_path = data_dir / filename
        if file_path.exists() and file_path.stat().st_size > 0:
            size_kb = file_path.stat().st_size // 1024
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            logger.info(f"âœ… {filename} ({size_kb}KB) - {mod_time.strftime('%H:%M:%S')}")
            valid_files += 1
        else:
            logger.error(f"âŒ {filename} - Missing or empty")
    
    return valid_files == len(required_files)

def generate_final_summary():
    """ìµœì¢… íŒŒì´í”„ë¼ì¸ ìš”ì•½ ìƒì„±"""
    logger.info("ğŸ“Š Generating final pipeline summary...")
    
    try:
        import json
        
        summary = []
        summary.append("=" * 80)
        summary.append(f"NPB ì™„ì „ ìƒˆë¡œìš´ Pipeline ì™„ë£Œ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        summary.append("=" * 80)
        summary.append("")
        summary.append("ğŸš€ Pipeline í”Œë¡œìš°:")
        summary.append("  ì›¹ í¬ë¡¤ë§ â†’ TXT ì €ì¥ â†’ JavaScript ì²˜ë¦¬ â†’ JSON ì €ì¥")
        summary.append("  ğŸ“ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ íŒŒì´í”„ë¼ì¸!")
        summary.append("")
        
        # JSON ë°ì´í„° ê²€ì¦
        data_dir = project_root / 'data'
        
        # ìˆœìœ„í‘œ ì •ë³´
        standings_file = data_dir / 'standings.json'
        if standings_file.exists():
            with open(standings_file, 'r', encoding='utf-8') as f:
                standings_data = json.load(f)
                central_teams = len(standings_data.get('central_league', {}).get('standings', []))
                pacific_teams = len(standings_data.get('pacific_league', {}).get('standings', []))
                summary.append(f"ğŸ“Š ìˆœìœ„ ê³„ì‚°: ì„¼íŠ¸ëŸ´ {central_teams}íŒ€, í¼ì‹œí”½ {pacific_teams}íŒ€")
        
        # ê²½ê¸° ì •ë³´
        games_file = data_dir / 'games.json'
        if games_file.exists():
            with open(games_file, 'r', encoding='utf-8') as f:
                games_data = json.load(f)
                total_games = len(games_data) if isinstance(games_data, list) else 0
                draws = sum(1 for game in games_data if game.get('is_draw', False)) if isinstance(games_data, list) else 0
                summary.append(f"âš¾ ê²½ê¸° ë°ì´í„°: {total_games}ê²½ê¸° (ë¬´ìŠ¹ë¶€ {draws}ê²½ê¸°)")
        
        # ëŒ€ì‹œë³´ë“œ ì •ë³´
        dashboard_file = data_dir / 'dashboard.json'
        if dashboard_file.exists():
            with open(dashboard_file, 'r', encoding='utf-8') as f:
                dashboard_data = json.load(f)
                today_games = dashboard_data.get('season_stats', {}).get('today_games', 0)
                week_games = dashboard_data.get('season_stats', {}).get('week_games', 0)
                summary.append(f"ğŸ“ˆ ëŒ€ì‹œë³´ë“œ: ì˜¤ëŠ˜ {today_games}ê²½ê¸°, ì´ë²ˆì£¼ {week_games}ê²½ê¸°")
        
        summary.append("")
        summary.append("ğŸ¯ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ:")
        summary.append("  âœ… ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘")
        summary.append("  âœ… TXT í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥")
        summary.append("  âœ… JavaScriptë¡œ ìˆœìœ„ ê³„ì‚°")
        summary.append("  âœ… JSON íŒŒì¼ ìƒì„± (index.html í˜¸í™˜)")
        summary.append("  âœ… ì™¸ë¶€ ì˜ì¡´ì„± ìµœì†Œí™”")
        summary.append("")
        summary.append("ğŸŒ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
        summary.append("  ì‹¤í–‰: ./run_html.sh")
        
        summary_text = '\n'.join(summary)
        logger.info(f"Final Pipeline Summary:\n{summary_text}")
        
        # ìš”ì•½ì„ íŒŒì¼ë¡œë„ ì €ì¥
        summary_file = project_root / 'logs' / 'new_pipeline' / f"final_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        
        logger.info(f"ğŸ“‹ Final summary saved to: {summary_file}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Summary generation error: {e}")
        return False

def main():
    """ìƒˆë¡œìš´ Pipeline ë©”ì¸ ì‹¤í–‰"""
    global logger
    logger = setup_logging()
    
    logger.info("ğŸš€ Starting NPB NEW PIPELINE")
    logger.info("ğŸ”„ Flow: Web Crawling â†’ TXT â†’ JavaScript â†’ JSON")
    
    success_count = 0
    total_steps = 4
    
    # ì¸ì íŒŒì‹±
    args = sys.argv[1:]
    skip_crawl = False
    if '--skip-crawl' in args:
        skip_crawl = True
        args = [a for a in args if a != '--skip-crawl']

    use_legacy = False
    if '--legacy-crawler' in args:
        use_legacy = True
        args = [a for a in args if a != '--legacy-crawler']
    
    # í¬ë¡¤ë§ ëª¨ë“œ ì„¤ì •
    crawl_mode = "7"  # ê¸°ë³¸ 7ì¼
    if len(args) > 0:
        if args[0] == '--full-season':
            crawl_mode = "full-season"
        elif args[0] == '--test':
            crawl_mode = "3"
        elif args[0] == '--quick':
            crawl_mode = "1"
        else:
            try:
                crawl_mode = str(int(args[0]))
            except ValueError:
                logger.error(f"Invalid argument: {args[0]}")
                sys.exit(1)
    
    # Step 1: ì›¹ í¬ë¡¤ë§ (TXT ì €ì¥)
    if skip_crawl:
        logger.info("Step 1/4: Skipping web crawl (--skip-crawl)")
        success_count += 1
    else:
        if crawl_mode == "full-season":
            logger.info("Step 1/4: Full season web crawling (from March 28)")
        else:
            logger.info(f"Step 1/4: Web crawling ({crawl_mode} days)")
        if run_web_crawler(crawl_mode, use_legacy=use_legacy):
            success_count += 1
    
    # Step 2: TXT â†’ JSON ë³€í™˜ (JavaScript ì²˜ë¦¬)  
    logger.info("Step 2/4: TXT to JSON conversion")
    if convert_txt_to_json():
        success_count += 1
    
    # Step 3: ì¶œë ¥ íŒŒì¼ ê²€ì¦
    logger.info("Step 3/4: Output file validation")
    if validate_output_files():
        success_count += 1
    
    # Step 4: ìµœì¢… ìš”ì•½
    logger.info("Step 4/4: Final summary generation")
    if generate_final_summary():
        success_count += 1
    
    # ìµœì¢… ê²°ê³¼
    logger.info(f"ğŸ¯ NEW Pipeline completed: {success_count}/{total_steps} steps successful")
    
    if success_count == total_steps:
        logger.info("âœ… All pipeline steps completed successfully!")
        logger.info("ğŸŒ Ready for web service!")
        logger.info("ğŸš€ Run: ./run_html.sh")
        logger.info("ğŸ’¾ Simple & Fast!")
        sys.exit(0)
    else:
        logger.error(f"âŒ Pipeline failed: {total_steps - success_count} steps failed")
        sys.exit(1)

if __name__ == "__main__":
    main()
