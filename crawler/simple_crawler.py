#!/usr/bin/env python3
"""
NPB Simple Crawler - ì§ì ‘ TXT ì €ì¥ ë°©ì‹
í¬ë¡¤ë§ â†’ TXT ì €ì¥ â†’ JavaScript ì²˜ë¦¬ â†’ JSON
"""

try:
    import requests
    from bs4 import BeautifulSoup
    CRAWLING_ENABLED = True
except ImportError:
    print("âš ï¸ Web crawling dependencies not available (requests, beautifulsoup4)")
    print("ğŸ“„ Using existing data conversion instead...")
    CRAWLING_ENABLED = False
    requests = None
    BeautifulSoup = None

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import time
import logging
import sys

class SimpleCrawler:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data" / "simple"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.setup_logging()
        
        # NPB íŒ€ ì •ë³´ (ì›¹ì‚¬ì´íŠ¸ í‘œì‹œëª… ê¸°ì¤€)
        self.teams = {
            # ì„¼íŠ¸ëŸ´ë¦¬ê·¸
            'ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„': {'id': 1, 'abbr': 'YOG', 'name': 'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„', 'league': 'Central'},
            'å·¨äºº': {'id': 1, 'abbr': 'YOG', 'name': 'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„', 'league': 'Central'},
            'å·¨': {'id': 1, 'abbr': 'YOG', 'name': 'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„', 'league': 'Central'},  # NPB ì¶•ì•½í˜•
            'é˜ªç¥': {'id': 2, 'abbr': 'HAN', 'name': 'é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹', 'league': 'Central'},
            'ç¥': {'id': 2, 'abbr': 'HAN', 'name': 'é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹', 'league': 'Central'},  # NPB 1ë¬¸ì í‘œê¸°
            'é˜ª': {'id': 2, 'abbr': 'HAN', 'name': 'é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹', 'league': 'Central'},  # NPB ì¶•ì•½í˜•
            'ï¼¤ï½…ï¼®ï¼¡': {'id': 3, 'abbr': 'YDB', 'name': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'league': 'Central'},
            'DeNA': {'id': 3, 'abbr': 'YDB', 'name': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'league': 'Central'},
            'ãƒ‡': {'id': 3, 'abbr': 'YDB', 'name': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'league': 'Central'},
            'ï¼¤': {'id': 3, 'abbr': 'YDB', 'name': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'league': 'Central'},  # NPB ì¶•ì•½í˜•
            'ä¸­æ—¥': {'id': 5, 'abbr': 'CHU', 'name': 'ä¸­æ—¥ãƒ‰ãƒ©ã‚´ãƒ³ã‚º', 'league': 'Central'},
            'ä¸­': {'id': 5, 'abbr': 'CHU', 'name': 'ä¸­æ—¥ãƒ‰ãƒ©ã‚´ãƒ³ã‚º', 'league': 'Central'},  # NPB ì¶•ì•½í˜•
            'åºƒå³¶': {'id': 4, 'abbr': 'HIR', 'name': 'åºƒå³¶æ±æ´‹ã‚«ãƒ¼ãƒ—', 'league': 'Central'},
            'åºƒ': {'id': 4, 'abbr': 'HIR', 'name': 'åºƒå³¶æ±æ´‹ã‚«ãƒ¼ãƒ—', 'league': 'Central'},  # NPB ì¶•ì•½í˜•
            'ãƒ¤ã‚¯ãƒ«ãƒˆ': {'id': 6, 'abbr': 'YAK', 'name': 'æ±äº¬ãƒ¤ã‚¯ãƒ«ãƒˆã‚¹ãƒ¯ãƒ­ãƒ¼ã‚º', 'league': 'Central'},
            'ãƒ¤': {'id': 6, 'abbr': 'YAK', 'name': 'æ±äº¬ãƒ¤ã‚¯ãƒ«ãƒˆã‚¹ãƒ¯ãƒ­ãƒ¼ã‚º', 'league': 'Central'},  # NPB ì¶•ì•½í˜•
            
            # í¼ì‹œí”½ë¦¬ê·¸
            'ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯': {'id': 7, 'abbr': 'SOF', 'name': 'ç¦å²¡ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ãƒ›ãƒ¼ã‚¯ã‚¹', 'league': 'Pacific'},
            'ã‚½': {'id': 7, 'abbr': 'SOF', 'name': 'ç¦å²¡ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ãƒ›ãƒ¼ã‚¯ã‚¹', 'league': 'Pacific'},  # NPB ì¶•ì•½í˜•
            'ãƒ­ãƒƒãƒ†': {'id': 8, 'abbr': 'LOT', 'name': 'åƒè‘‰ãƒ­ãƒƒãƒ†ãƒãƒªãƒ¼ãƒ³ã‚º', 'league': 'Pacific'},
            'ãƒ­': {'id': 8, 'abbr': 'LOT', 'name': 'åƒè‘‰ãƒ­ãƒƒãƒ†ãƒãƒªãƒ¼ãƒ³ã‚º', 'league': 'Pacific'},  # NPB ì¶•ì•½í˜•
            'æ¥½å¤©': {'id': 9, 'abbr': 'RAK', 'name': 'æ±åŒ—æ¥½å¤©ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¤ãƒ¼ã‚°ãƒ«ã‚¹', 'league': 'Pacific'},
            'æ¥½': {'id': 9, 'abbr': 'RAK', 'name': 'æ±åŒ—æ¥½å¤©ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¤ãƒ¼ã‚°ãƒ«ã‚¹', 'league': 'Pacific'},  # NPB ì¶•ì•½í˜•
            'ã‚ªãƒªãƒƒã‚¯ã‚¹': {'id': 10, 'abbr': 'ORI', 'name': 'ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º', 'league': 'Pacific'},
            'ã‚ª': {'id': 10, 'abbr': 'ORI', 'name': 'ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º', 'league': 'Pacific'},  # NPB ì¶•ì•½í˜•
            'è¥¿æ­¦': {'id': 11, 'abbr': 'SEI', 'name': 'åŸ¼ç‰è¥¿æ­¦ãƒ©ã‚¤ã‚ªãƒ³ã‚º', 'league': 'Pacific'},
            'è¥¿': {'id': 11, 'abbr': 'SEI', 'name': 'åŸ¼ç‰è¥¿æ­¦ãƒ©ã‚¤ã‚ªãƒ³ã‚º', 'league': 'Pacific'},  # NPB ì¶•ì•½í˜•
            'æ—¥æœ¬ãƒãƒ ': {'id': 12, 'abbr': 'NIP', 'name': 'åŒ—æµ·é“æ—¥æœ¬ãƒãƒ ãƒ•ã‚¡ã‚¤ã‚¿ãƒ¼ã‚º', 'league': 'Pacific'},
            'æ—¥': {'id': 12, 'abbr': 'NIP', 'name': 'åŒ—æµ·é“æ—¥æœ¬ãƒãƒ ãƒ•ã‚¡ã‚¤ã‚¿ãƒ¼ã‚º', 'league': 'Pacific'}  # NPB ì¶•ì•½í˜•
        }

        # í™ˆíŒ€ ê¸°ë³¸ êµ¬ì¥ ë§¤í•‘ (í‘œì‹œìš© ì¶”ì •ì¹˜)
        self.default_stadium_by_abbr = {
            'YOG': 'æ±äº¬ãƒ‰ãƒ¼ãƒ ',
            'HAN': 'é˜ªç¥ç”²å­åœ’çƒå ´',
            'CHU': 'ãƒãƒ³ãƒ†ãƒªãƒ³ãƒ‰ãƒ¼ãƒ  ãƒŠã‚´ãƒ¤',
            'YDB': 'æ¨ªæµœã‚¹ã‚¿ã‚¸ã‚¢ãƒ ',
            'HIR': 'MAZDA Zoom-Zoom ã‚¹ã‚¿ã‚¸ã‚¢ãƒ åºƒå³¶',
            'YAK': 'æ˜æ²»ç¥å®®é‡çƒå ´',
            'SOF': 'ç¦å²¡PayPayãƒ‰ãƒ¼ãƒ ',
            'LOT': 'ZOZOãƒãƒªãƒ³ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ',
            'SEI': 'ãƒ™ãƒ«ãƒ¼ãƒŠãƒ‰ãƒ¼ãƒ ',
            'ORI': 'äº¬ã‚»ãƒ©ãƒ‰ãƒ¼ãƒ å¤§é˜ª',
            'NIP': 'ã‚¨ã‚¹ã‚³ãƒ³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰HOKKAIDO',
            'RAK': 'æ¥½å¤©ãƒ¢ãƒã‚¤ãƒ«ãƒ‘ãƒ¼ã‚¯å®®åŸ',
        }
    
    def setup_logging(self):
        log_dir = self.project_root / "logs" / "simple_crawler"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = log_dir / f"crawler_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('simple_crawler')
    
    def get_team_info(self, team_name):
        """íŒ€ëª…ìœ¼ë¡œ íŒ€ ì •ë³´ ì°¾ê¸°"""
        for key, info in self.teams.items():
            if key in team_name:
                return info
        return None
    
    def convert_existing_data_to_txt(self):
        """ê¸°ì¡´ JSON â†’ TXT ì—­ë³€í™˜ (í¬ë¡¤ë§ ë¶ˆê°€ ì‹œ fallback)"""
        self.logger.info("ğŸ“„ Converting existing JSON data to TXT format (fallback)...")
        
        try:
            # Use existing json_to_txt_converter script
            json_to_txt = self.project_root / 'scripts' / 'json_to_txt_converter.py'
            if json_to_txt.exists():
                result = os.system(f"cd {self.project_root} && python3 {json_to_txt}")
                if result == 0:
                    self.logger.info("âœ… JSON to TXT conversion completed")
                    return True
                else:
                    self.logger.error("âŒ JSON to TXT conversion failed")
                    return False
            else:
                # If converter not present, try to proceed with existing TXT files
                games_txt = self.data_dir / 'games_raw.txt'
                teams_txt = self.data_dir / 'teams_raw.txt'
                if games_txt.exists() and teams_txt.exists():
                    self.logger.info("âš ï¸ Converter not found, but TXT files already exist. Proceeding.")
                    return True
                self.logger.error("âŒ JSON to TXT converter not found and TXT files missing")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ JSONâ†’TXT conversion error: {e}")
            return False

    def crawl_date(self, target_date):
        """íŠ¹ì • ë‚ ì§œì˜ ê²½ê¸° ê²°ê³¼ í¬ë¡¤ë§"""
        if not CRAWLING_ENABLED:
            return []  # Skip actual crawling if dependencies unavailable
            
        self.logger.info(f"ğŸ” Crawling: {target_date.strftime('%Y-%m-%d')}")
        
        # 1. NPB ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ê²½ê¸° ì •ë³´ ì‹œë„
        games = self.crawl_game_detail(target_date)
        
        # 2. NPBì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìœ¼ë©´ ë‹›ì¹¸ìŠ¤í¬ì¸ ì—ì„œ ì‹œë„
        if not games:
            games = self.crawl_from_nikkansports(target_date)
        
        # 3. ê²½ê¸° ìƒíƒœ ë¡œê·¸ ì¶œë ¥
        for game in games:
            if game.get('status') == 'completed':
                self.logger.info(f"âœ… Completed: {game['away_team_abbr']} {game.get('away_score', 0)}-{game.get('home_score', 0)} {game['home_team_abbr']}")
            elif game.get('status') == 'postponed':
                self.logger.info(f"â¸ï¸ Postponed: {game['away_team_abbr']} vs {game['home_team_abbr']}")
            else:
                self.logger.info(f"ğŸ“… Scheduled: {game['away_team_abbr']} vs {game['home_team_abbr']}")
        
        self.logger.info(f"âœ… Found {len(games)} games on {target_date.strftime('%Y-%m-%d')}")
        return games
        
    def crawl_from_nikkansports(self, target_date):
        """ë‹›ì¹¸ìŠ¤í¬ì¸ ì—ì„œ ê²½ê¸° ê²°ê³¼ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)"""
        # URL í˜•ì‹: https://www.nikkansports.com/baseball/professional/score/2025/pf-score-20250328.html
        date_str = target_date.strftime("%Y%m%d")
        year = target_date.strftime("%Y")
        url = f"https://www.nikkansports.com/baseball/professional/score/{year}/pf-score-{date_str}.html"
        
        self.logger.info(f"ğŸ“° Trying Nikkansports: {target_date.strftime('%Y-%m-%d')}")
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            # Use raw content so BeautifulSoup can detect meta charset correctly
            soup = BeautifulSoup(response.content, 'html.parser')
            games = []
            
            # scoreTable í´ë˜ìŠ¤ì˜ í…Œì´ë¸”ë“¤ì—ì„œ ê²½ê¸° ê²°ê³¼ íŒŒì‹±
            score_tables = soup.find_all('table', class_='scoreTable')
            
            for table in score_tables:
                try:
                    rows = table.find_all('tr')
                    if len(rows) < 3:  # í—¤ë” + 2íŒ€ ìµœì†Œ í•„ìš”
                        continue
                    
                    # íŒ€ëª… ì¶”ì¶œ (ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ í–‰)
                    away_row = rows[1]  # ì²« ë²ˆì§¸ íŒ€ (away)
                    home_row = rows[2]  # ë‘ ë²ˆì§¸ íŒ€ (home)
                    
                    # íŒ€ëª…ì—ì„œ ê³µë°± ì œê±°í•˜ê³  ë§¤í•‘
                    away_team_text = away_row.find('td', class_='team').get_text(strip=True).replace('\xa0', '')
                    home_team_text = home_row.find('td', class_='team').get_text(strip=True).replace('\xa0', '')
                    
                    away_team = self.get_team_info(away_team_text)
                    home_team = self.get_team_info(home_team_text)
                    
                    if not away_team or not home_team:
                        self.logger.warning(f"âš ï¸ Team not found: {away_team_text} vs {home_team_text}")
                        continue
                    
                    # totalScore í´ë˜ìŠ¤ì—ì„œ ì´ì  ì¶”ì¶œ
                    away_score_cell = away_row.find('td', class_='totalScore')
                    home_score_cell = home_row.find('td', class_='totalScore')
                    
                    if not away_score_cell or not home_score_cell:
                        self.logger.warning(f"âš ï¸ Could not find totalScore cells")
                        continue
                    
                    # ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    away_score_text = away_score_cell.get_text(strip=True)
                    home_score_text = home_score_cell.get_text(strip=True)

                    # ìˆ«ì íŒŒì‹± ìœ í‹¸ (í’€ì™€ì´ë“œ ìˆ«ì í¬í•¨). ì‹¤íŒ¨ ì‹œ None ë°˜í™˜í•˜ì—¬ ìŠ¤í‚µ
                    def convert_jp_number(text: str):
                        if text is None:
                            return None
                        # ì „ê° ìˆ«ìë¥¼ ë°˜ê°ìœ¼ë¡œ ì¹˜í™˜
                        trans = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
                        t = text.translate(trans)
                        # í”í•œ ë¹„ìˆ«ì ê¸°í˜¸ ì œê±° (ëŒ€ì‰¬, ê³µë°±)
                        t = t.replace('\u2014', '-')\
                             .replace('\u2013', '-')\
                             .replace('ï¼', '-')\
                             .replace('â€”', '-')\
                             .strip()
                        # ëª…ë°±í•œ ë¹„ì™„ë£Œ/ì·¨ì†Œ í‘œì‹œ ì²˜ë¦¬: ìˆ«ìê°€ ì—†ìœ¼ë©´ None
                        if not any(ch.isdigit() for ch in t):
                            return None
                        # ìˆ«ìë§Œ ë‚¨ê¸°ê¸° (ì˜ˆ: '10' ê·¸ëŒ€ë¡œ, 'X' ë“± ì œê±°)
                        cleaned = ''.join(ch for ch in t if ch.isdigit())
                        if cleaned == '':
                            return None
                        try:
                            return int(cleaned)
                        except Exception:
                            return None

                    away_score = convert_jp_number(away_score_text)
                    home_score = convert_jp_number(home_score_text)

                    # ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨(ë¯¸ì§„í–‰/ì¤‘ì§€ ë“±)ì¸ ê²½ìš° ìŠ¤í‚µ
                    if away_score is None or home_score is None:
                        self.logger.info(
                            f"â­ï¸  Skipping unparsed/unfinished game: {away_team['abbr']} vs {home_team['abbr']} (away='{away_score_text}', home='{home_score_text}')"
                        )
                        continue
                    
                    # ë¦¬ê·¸ íŒë‹¨ (íŒ€ ì •ë³´ì—ì„œ)
                    league = away_team['league']
                    
                    # ê²½ê¸° ìƒíƒœ ì •ë³´ ì¶”ì¶œ
                    game_status_info = self.extract_game_status(table)
                    
                    # ê²½ê¸° ì •ë³´
                    game = {
                        'date': target_date.strftime('%Y-%m-%d'),
                        'home_team_id': home_team['id'],
                        'home_team_name': home_team['name'],
                        'home_team_abbr': home_team['abbr'],
                        'away_team_id': away_team['id'],
                        'away_team_name': away_team['name'],
                        'away_team_abbr': away_team['abbr'],
                        'home_score': home_score,
                        'away_score': away_score,
                        'league': league,
                        'status': game_status_info['status'],
                        'inning': game_status_info['inning'],
                        'inning_half': game_status_info['inning_half'],
                        'game_time': game_status_info['game_time'],
                        'inning_scores': game_status_info.get('inning_scores', []),
                        'is_draw': home_score == away_score,  # ì‹¤ì œ ë™ì ë§Œ ë¬´ìŠ¹ë¶€
                        'winner': 'home' if home_score > away_score else ('away' if away_score > home_score else 'draw')
                    }
                    
                    games.append(game)
                    status_text = f" [{game['status'].upper()}]" if game['status'] != 'completed' else ""
                    self.logger.info(f"âœ… Parsed: {away_team['abbr']} {away_score}-{home_score} {home_team['abbr']}{status_text}")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Failed to parse table: {e}")
                    continue
            
            return games
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to crawl from Nikkansports {target_date.strftime('%Y-%m-%d')}: {e}")
            return []
    
    def extract_game_status(self, table):
        """ê²½ê¸° ìƒíƒœ ì •ë³´ ì¶”ì¶œ (ì´ë‹, ì§„í–‰ìƒí™©, ì‹œê°„ ë“±)"""
        status_info = {
            'status': 'scheduled',  # ê¸°ë³¸ê°’: ì˜ˆì • (ëª…í™•í•œ ì™„ë£Œ í‘œì‹œê°€ ìˆì„ ë•Œë§Œ completedë¡œ ë³€ê²½)
            'inning': None,
            'inning_half': None,  # 'top' ë˜ëŠ” 'bottom'
            'game_time': None,
            'inning_scores': [],  # ì´ë‹ë³„ ìŠ¤ì½”ì–´
            'current_runners': None,  # ì£¼ì ìƒí™©
            'balls_strikes': None,  # ë³¼ì¹´ìš´íŠ¸
            'outs': None  # ì•„ì›ƒ ì¹´ìš´íŠ¸
        }
        
        try:
            # ê²½ê¸° ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìš”ì†Œë“¤ì„ ì°¾ì•„ì„œ íŒŒì‹±
            
            # 1. í—¤ë”ì—ì„œ ê²½ê¸° ì‹œê°„ì´ë‚˜ ìƒíƒœ ì •ë³´ ì°¾ê¸°
            header_row = table.find('tr')
            if header_row:
                header_text = header_row.get_text(strip=True)
                
                # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: "18:00 é–‹å§‹" ë“±)
                import re
                time_match = re.search(r'(\d{1,2}):(\d{2})', header_text)
                if time_match:
                    status_info['game_time'] = f"{time_match.group(1)}:{time_match.group(2)}"
            
            # 2. ê²½ê¸° ì§„í–‰ ìƒíƒœ í™•ì¸
            status_elements = table.find_all(['td', 'th'], class_=['status', 'inning', 'gameStatus'])
            for elem in status_elements:
                text = elem.get_text(strip=True)
                
                # ì™„ë£Œ ìƒíƒœë§Œ í™•ì¸ (ë‹¤ì–‘í•œ ì™„ë£Œ í‘œí˜„ ì¶”ê°€)
                completion_keywords = ['è©¦åˆçµ‚äº†', 'çµ‚äº†', 'ã‚²ãƒ¼ãƒ çµ‚äº†', 'GAME SET', 'FINAL', 'æœ€çµ‚']
                if any(keyword in text for keyword in completion_keywords):
                    status_info['status'] = 'completed'
                
                # ì—°ê¸°/ì¤‘ì§€ ìƒíƒœ í™•ì¸
                elif any(keyword in text for keyword in ['é›¨å¤©ä¸­æ­¢', 'ä¸­æ­¢', 'å»¶æœŸ', 'ã‚µã‚¹ãƒšãƒ³ãƒ‡ãƒƒãƒ‰']):
                    status_info['status'] = 'postponed'
                
                # ì´ë‹ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: "7å›è¡¨", "9å›è£", "å»¶é•·10å›")
                inning_match = re.search(r'(?:å»¶é•·)?(\d+)å›([è¡¨è£])?', text)
                if inning_match:
                    status_info['inning'] = int(inning_match.group(1))
                    if inning_match.group(2):
                        status_info['inning_half'] = 'top' if inning_match.group(2) == 'è¡¨' else 'bottom'
            
            # 3. ìŠ¤ì½”ì–´ë³´ë“œì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            score_cells = table.find_all('td', class_='totalScore')
            for cell in score_cells:
                # ìŠ¤ì½”ì–´ê°€ í™•ì •ëœ ê²½ìš°ë§Œ ì™„ë£Œë¡œ ì²˜ë¦¬
                # ì§„í–‰ì¤‘ í‘œì‹œëŠ” ë¬´ì‹œ
            
            # 4. ì´ë‹ë³„ ìŠ¤ì½”ì–´ ì¶”ì¶œ
            inning_cells = table.find_all('td', class_=['inning', 'inningScore'])
            for cell in inning_cells:
                score_text = cell.get_text(strip=True)
                if score_text.isdigit():
                    status_info['inning_scores'].append(int(score_text))
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ Could not extract game status: {e}")
            
        return status_info
    
    def save_games_to_txt(self, games, filename="games_raw.txt"):
        """ê²½ê¸° ê²°ê³¼ë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥
        upcoming_games_raw.txtì˜ ê²½ìš°, êµ¬ì¥/ê²½ê¸°ì‹œê°„ í•„ë“œë¥¼ ëì— ì¶”ê°€í•˜ê³  ì „ì²´ íŒŒì¼ì„ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
        """
        if not games:
            return
        
        file_path = self.data_dir / filename
        is_upcoming = (filename == "upcoming_games_raw.txt")
        # upcomingëŠ” í•­ìƒ ë®ì–´ì“°ê¸°(ìµœì‹  ìƒíƒœ ìœ ì§€), ë‚˜ë¨¸ì§€ëŠ” append + dedup
        if is_upcoming:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("# NPB_SCHEDULED_GAMES_DATA\n")
                f.write(f"# UPDATED: {datetime.now().isoformat()}\n") 
                f.write("# FORMAT: DATE|HOME_ID|HOME_ABBR|HOME_NAME|AWAY_ID|AWAY_ABBR|AWAY_NAME|HOME_SCORE|AWAY_SCORE|LEAGUE|STATUS|IS_DRAW|STADIUM|GAME_TIME\n")
                f.write("# NOTE: HOME_SCORE and AWAY_SCORE are 'NULL' for scheduled games. STADIUM/GAME_TIME may be estimates.\n")

                for game in games:
                    home_score = 'NULL' if game.get('home_score') is None else str(game['home_score'])
                    away_score = 'NULL' if game.get('away_score') is None else str(game['away_score'])
                    
                    if is_upcoming:
                        stadium = game.get('stadium')
                        if not stadium:
                            abbr = game.get('home_team_abbr')
                            stadium = self.default_stadium_by_abbr.get(abbr, '')
                        game_time = game.get('game_time', '')
                        line = "|".join([
                            game['date'],
                            str(game['home_team_id']),
                            game['home_team_abbr'],
                            game['home_team_name'],
                            str(game['away_team_id']),
                            game['away_team_abbr'],
                            game['away_team_name'],
                            home_score,
                            away_score,
                            game['league'],
                            game.get('status', 'scheduled'),
                            '1' if game.get('is_draw') else '0',
                            stadium,
                            game_time,
                        ])
                    
                    f.write(line + '\n')
                
                self.logger.info(f"ğŸ“„ Rewrote {file_path} with {len(games)} scheduled games")
            return

        # ê¸°ì¡´ íŒŒì¼ ì½ê¸° (ì¤‘ë³µ ë°©ì§€) - ì™„ë£Œ ê²½ê¸°ìš©
        existing_games = []
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith('#') or not line.strip():
                            continue
                        existing_games.append(line.strip())
            except Exception as e:
                self.logger.warning(f"Failed to read existing file: {e}")
        
        new_lines = []
        existing_set = set(existing_games)
        for game in games:
            home_score = 'NULL' if game['home_score'] is None else str(game['home_score'])
            away_score = 'NULL' if game['away_score'] is None else str(game['away_score'])
            line = "|".join([
                game['date'],
                str(game['home_team_id']),
                game['home_team_abbr'], 
                game['home_team_name'],
                str(game['away_team_id']),
                game['away_team_abbr'],
                game['away_team_name'],
                home_score,
                away_score,
                game['league'],
                game.get('status', 'completed'),
                '1' if game.get('is_draw') else '0'
            ])
            if line not in existing_set:
                new_lines.append(line)
                existing_set.add(line)
        
        if new_lines:
            with open(file_path, 'a', encoding='utf-8') as f:
                if file_path.stat().st_size == 0:
                    f.write("# NPB_GAMES_DATA\n")
                    f.write(f"# UPDATED: {datetime.now().isoformat()}\n") 
                    f.write("# FORMAT: DATE|HOME_ID|HOME_ABBR|HOME_NAME|AWAY_ID|AWAY_ABBR|AWAY_NAME|HOME_SCORE|AWAY_SCORE|LEAGUE|STATUS|IS_DRAW\n")
                for line in new_lines:
                    f.write(line + '\n')
            self.logger.info(f"ğŸ“„ Saved {len(new_lines)} new games to {file_path}")
        else:
            self.logger.info("ğŸ“„ No new games to save")
    
    def save_teams_to_txt(self):
        """íŒ€ ì •ë³´ë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥"""
        file_path = self.data_dir / "teams_raw.txt"
        
        lines = []
        lines.append("# NPB_TEAMS_DATA")
        lines.append(f"# UPDATED: {datetime.now().isoformat()}")
        lines.append("# FORMAT: TEAM_ID|TEAM_ABBR|TEAM_NAME|LEAGUE")
        
        # ì¤‘ë³µ ì œê±°: ê³ ì • 12íŒ€ë§Œ ì¶œë ¥ (id ì˜¤ë¦„ì°¨ìˆœ)
        canonical = {
            1: {'id':1,'abbr':'YOG','name':'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„','league':'Central'},
            2: {'id':2,'abbr':'HAN','name':'é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹','league':'Central'},
            3: {'id':3,'abbr':'YDB','name':'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º','league':'Central'},
            4: {'id':4,'abbr':'HIR','name':'åºƒå³¶æ±æ´‹ã‚«ãƒ¼ãƒ—','league':'Central'},
            5: {'id':5,'abbr':'CHU','name':'ä¸­æ—¥ãƒ‰ãƒ©ã‚´ãƒ³ã‚º','league':'Central'},
            6: {'id':6,'abbr':'YAK','name':'æ±äº¬ãƒ¤ã‚¯ãƒ«ãƒˆã‚¹ãƒ¯ãƒ­ãƒ¼ã‚º','league':'Central'},
            7: {'id':7,'abbr':'SOF','name':'ç¦å²¡ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ãƒ›ãƒ¼ã‚¯ã‚¹','league':'Pacific'},
            8: {'id':8,'abbr':'LOT','name':'åƒè‘‰ãƒ­ãƒƒãƒ†ãƒãƒªãƒ¼ãƒ³ã‚º','league':'Pacific'},
            9: {'id':9,'abbr':'RAK','name':'æ±åŒ—æ¥½å¤©ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¤ãƒ¼ã‚°ãƒ«ã‚¹','league':'Pacific'},
            10:{'id':10,'abbr':'ORI','name':'ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º','league':'Pacific'},
            11:{'id':11,'abbr':'SEI','name':'åŸ¼ç‰è¥¿æ­¦ãƒ©ã‚¤ã‚ªãƒ³ã‚º','league':'Pacific'},
            12:{'id':12,'abbr':'NIP','name':'åŒ—æµ·é“æ—¥æœ¬ãƒãƒ ãƒ•ã‚¡ã‚¤ã‚¿ãƒ¼ã‚º','league':'Pacific'},
        }
        for team_id in sorted(canonical.keys()):
            info = canonical[team_id]
            line = "|".join([
                str(info['id']),
                info['abbr'],
                info['name'],
                info['league']
            ])
            lines.append(line)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        self.logger.info(f"ğŸ“„ Saved teams to {file_path}")
    
    def crawl_full_season(self, start_date="2025-03-28"):
        """NPB ì‹œì¦Œ ì „ì²´ í¬ë¡¤ë§ (3ì›” 28ì¼ë¶€í„°)"""
        self.logger.info(f"ğŸš€ Starting full NPB season crawl from {start_date}...")
        
        if not CRAWLING_ENABLED:
            self.logger.info("ğŸ“„ Web crawling not available, using existing data...")
            # Use existing data instead
            if self.convert_existing_data_to_txt():
                self.save_teams_to_txt()
                return 1  # Success
            else:
                return 0
        
        all_games = []
        start = datetime.strptime(start_date, "%Y-%m-%d")
        today = datetime.now()
        
        # ë‹¹ì¼ ê²½ê¸°ë„ í¬í•¨ (ì™„ë£Œëœ ê²½ê¸°ëŠ” ìˆ˜ì§‘)
        end_date = today
        current_date = start
        total_days = (end_date - start).days + 1
        
        self.logger.info(f"ğŸ“… Crawling {total_days} days from {start_date} to {today.strftime('%Y-%m-%d')}")
        
        day_count = 0
        while current_date <= end_date:
            day_count += 1
            games = self.crawl_date(current_date)
            
            if games:
                all_games.extend(games)
                self.logger.info(f"ğŸ“… {current_date.strftime('%Y-%m-%d')}: {len(games)} games")
            else:
                # ê²½ê¸° ì—†ëŠ” ë‚  (íœ´ì‹ì¼)
                pass
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if day_count % 10 == 0 or day_count == total_days:
                progress = (day_count / total_days) * 100
                self.logger.info(f"ğŸ”„ Progress: {day_count}/{total_days} days ({progress:.1f}%)")
            
            current_date += timedelta(days=1)
            
            # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)
        
        # ê²½ê¸° ê²°ê³¼ ì €ì¥
        if all_games:
            self.save_games_to_txt(all_games)
        
        # íŒ€ ì •ë³´ ì €ì¥
        self.save_teams_to_txt()
        
        self.logger.info(f"ğŸ† **FULL SEASON CRAWL SUMMARY**")
        self.logger.info(f"Total games: {len(all_games)}")
        self.logger.info(f"Draws: {sum(1 for g in all_games if g['is_draw'])}")
        self.logger.info(f"Period: {start_date} to {today.strftime('%Y-%m-%d')}")
        
        # ì‹œì¦Œ í†µê³„
        if all_games:
            teams_count = {}
            for game in all_games:
                home_team = game['home_team_abbr']
                away_team = game['away_team_abbr']
                teams_count[home_team] = teams_count.get(home_team, 0) + 1
                teams_count[away_team] = teams_count.get(away_team, 0) + 1
            
            self.logger.info("ğŸ“Š **TEAM GAMES COUNT**:")
            for team, count in sorted(teams_count.items()):
                self.logger.info(f"  {team}: {count} games")
        
        return len(all_games)

    def crawl_multiple_days(self, days=7):
        """ì—¬ëŸ¬ ë‚ ì§œ í¬ë¡¤ë§ (ë˜ëŠ” ê¸°ì¡´ ë°ì´í„° ë³€í™˜)"""
        self.logger.info(f"ğŸš€ Starting simple crawl for last {days} days...")
        
        if not CRAWLING_ENABLED:
            self.logger.info("ğŸ“„ Web crawling not available, using existing data...")
            # Use existing data instead
            if self.convert_existing_data_to_txt():
                self.save_teams_to_txt()
                return 1  # Success
            else:
                return 0
        
        all_games = []
        today = datetime.now()
        
        for i in range(0, days):  # ì˜¤ëŠ˜ë¶€í„° ì‹œì‘  
            target_date = today - timedelta(days=i)
            games = self.crawl_date(target_date)
            all_games.extend(games)
            
            # ìš”ì²­ ê°„ê²©
            if i < days:
                time.sleep(1)
        
        # ê²½ê¸° ê²°ê³¼ ì €ì¥
        if all_games:
            self.save_games_to_txt(all_games)
        
        # íŒ€ ì •ë³´ ì €ì¥
        self.save_teams_to_txt()
        
        self.logger.info(f"ğŸ† **SIMPLE CRAWL SUMMARY**")
        self.logger.info(f"Total games: {len(all_games)}")
        self.logger.info(f"Draws: {sum(1 for g in all_games if g['is_draw'])}")
        
        return len(all_games)



    def crawl_upcoming_games(self, days_ahead=3):
        """ì˜ˆì • ê²½ê¸° í¬ë¡¤ë§ (NPB ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ)"""
        if not CRAWLING_ENABLED:
            return []
            
        self.logger.info(f"ğŸ” Crawling upcoming games for next {days_ahead} days...")
        
        all_upcoming_games = []
        today = datetime.now()
        
        for i in range(days_ahead):
            target_date = today + timedelta(days=i)
            games = self.crawl_upcoming_date(target_date)
            all_upcoming_games.extend(games)
            
            # ìš”ì²­ ê°„ê²©
            if i < days_ahead - 1:
                time.sleep(1)
        
        if all_upcoming_games:
            self.save_games_to_txt(all_upcoming_games, "upcoming_games_raw.txt")
        
        self.logger.info(f"ğŸ“… Found {len(all_upcoming_games)} upcoming games")
        return all_upcoming_games

    def crawl_game_detail(self, target_date):
        """íŠ¹ì • ë‚ ì§œì˜ ê²½ê¸° ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (NPB ê³µì‹ ì‚¬ì´íŠ¸)"""
        if not CRAWLING_ENABLED:
            return []
            
        # NPB ê³µì‹ ìŠ¤ì½”ì–´ í˜ì´ì§€ í˜•ì‹: https://npb.jp/scores/2025/0908/
        date_str = target_date.strftime("%m%d")
        year = target_date.year
        url = f"https://npb.jp/scores/{year}/{date_str}/"
        
        self.logger.info(f"ğŸ” Checking game details: {target_date.strftime('%Y-%m-%d')}")
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            games = []
            
            # NPB ìŠ¤ì½”ì–´ í˜ì´ì§€ì—ì„œ ê° ê²½ê¸° ë§í¬ ì°¾ê¸°
            game_links = soup.find_all('a', href=lambda x: x and '/scores/' in x and target_date.strftime('%Y') in x)
            
            for link in game_links:
                href = link.get('href')
                if href and 'detail' not in href:  # ìƒì„¸ í˜ì´ì§€ê°€ ì•„ë‹Œ ë©”ì¸ ê²½ê¸° ë§í¬ë§Œ
                    full_url = f"https://npb.jp{href}" if href.startswith('/') else href
                    
                    # ê° ê²½ê¸°ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
                    game_detail = self.crawl_single_game(full_url, target_date)
                    if game_detail:
                        games.append(game_detail)
                    
                    # ìš”ì²­ ê°„ê²©
                    time.sleep(0.5)
            
            return games
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to crawl games for {target_date.strftime('%Y-%m-%d')}: {e}")
            return []

    def crawl_single_game(self, game_url, target_date):
        """ë‹¨ì¼ ê²½ê¸°ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        try:
            response = requests.get(game_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê²½ê¸° ì •ë³´ ì¶”ì¶œ
            game_info = {
                'date': target_date.strftime('%Y-%m-%d'),
                'status': 'scheduled',  # ê¸°ë³¸ê°’ì„ scheduledë¡œ ì„¤ì •
                'inning': None,
                'inning_half': None,
                'inning_scores': {'away': [], 'home': []},
                'current_situation': {}
            }
            
            # 1. íŒ€ ì •ë³´ ë° ìµœì¢… ìŠ¤ì½”ì–´ ì¶”ì¶œ
            score_table = soup.find('table', class_='score-table')
            if score_table:
                rows = score_table.find_all('tr')
                if len(rows) >= 3:  # í—¤ë” + away + home
                    away_row = rows[1]
                    home_row = rows[2]
                    
                    # íŒ€ëª… ì¶”ì¶œ
                    away_team_cell = away_row.find('td', class_='team')
                    home_team_cell = home_row.find('td', class_='team')
                    
                    if away_team_cell and home_team_cell:
                        away_team_text = away_team_cell.get_text(strip=True)
                        home_team_text = home_team_cell.get_text(strip=True)
                        
                        away_team = self.get_team_info(away_team_text)
                        home_team = self.get_team_info(home_team_text)
                        
                        if away_team and home_team:
                            game_info['away_team_id'] = away_team['id']
                            game_info['away_team_abbr'] = away_team['abbr']
                            game_info['away_team_name'] = away_team['name']
                            game_info['home_team_id'] = home_team['id']
                            game_info['home_team_abbr'] = home_team['abbr']
                            game_info['home_team_name'] = home_team['name']
                            game_info['league'] = away_team['league']
                            
                            # ìµœì¢… ìŠ¤ì½”ì–´ ì¶”ì¶œ
                            away_total = away_row.find('td', class_='total')
                            home_total = home_row.find('td', class_='total')
                            
                            if away_total and home_total:
                                away_score_text = away_total.get_text(strip=True)
                                home_score_text = home_total.get_text(strip=True)
                                
                                # ìŠ¤ì½”ì–´ ë°ì´í„°ëŠ” í•­ìƒ ìˆ˜ì§‘ (ì§„í–‰ì¤‘ì´ë“  ì™„ë£Œë“ )
                                try:
                                    game_info['away_score'] = int(away_score_text)
                                    game_info['home_score'] = int(home_score_text)
                                    game_info['is_draw'] = game_info['away_score'] == game_info['home_score']
                                    game_info['winner'] = 'home' if game_info['home_score'] > game_info['away_score'] else ('away' if game_info['away_score'] > game_info['home_score'] else 'draw')
                                except ValueError:
                                    self.logger.warning(f"âš ï¸ Could not parse scores: away='{away_score_text}', home='{home_score_text}'")
                                    return None
                            
                            # ì´ë‹ë³„ ìŠ¤ì½”ì–´ ì¶”ì¶œ
                            inning_cells_away = away_row.find_all('td', class_='inning')
                            inning_cells_home = home_row.find_all('td', class_='inning')
                            
                            for cell in inning_cells_away:
                                score_text = cell.get_text(strip=True)
                                if score_text.isdigit():
                                    game_info['inning_scores']['away'].append(int(score_text))
                                elif score_text == 'X':
                                    game_info['inning_scores']['away'].append(None)  # í•˜ìœ„íŒ€ 9íšŒë§ì€ X
                            
                            for cell in inning_cells_home:
                                score_text = cell.get_text(strip=True)
                                if score_text.isdigit():
                                    game_info['inning_scores']['home'].append(int(score_text))
                                elif score_text == 'X':
                                    game_info['inning_scores']['home'].append(None)
            
            # 2. ê²½ê¸° ìƒíƒœ ì •ë³´ ì¶”ì¶œ
            status_section = soup.find('div', class_=['game-status'])
            if status_section:
                status_text = status_section.get_text(strip=True)
                
                # ê²½ê¸° ì™„ë£Œ ìƒíƒœë§Œ í™•ì¸ (ì§„í–‰ì¤‘ì´ë©´ ìƒíƒœ ë³€ê²½ ì•ˆí•¨)
                completion_keywords = ['è©¦åˆçµ‚äº†', 'çµ‚äº†', 'ã‚²ãƒ¼ãƒ çµ‚äº†', 'GAME SET', 'FINAL', 'æœ€çµ‚', 'çµæœ']
                if any(keyword in status_text for keyword in completion_keywords):
                    game_info['status'] = 'completed'
                elif any(keyword in status_text for keyword in ['å»¶æœŸ', 'ä¸­æ­¢', 'é›¨å¤©ä¸­æ­¢']):
                    game_info['status'] = 'postponed'
                # ì§„í–‰ì¤‘ì´ê±°ë‚˜ ê¸°íƒ€ ìƒíƒœë©´ ê¸°ë³¸ê°’(scheduled) ìœ ì§€
            
            # 3. ì¶”ê°€ ê²Œì„ ì‹œê°„ ì •ë³´
            game_time_elem = soup.find(['span', 'div'], class_=['game-time', 'start-time'])
            if game_time_elem:
                game_info['game_time'] = game_time_elem.get_text(strip=True)
            
            return game_info
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Failed to crawl single game: {game_url} - {e}")
            return None

    def crawl_upcoming_date(self, target_date):
        """íŠ¹ì • ë‚ ì§œì˜ ì˜ˆì • ê²½ê¸° í¬ë¡¤ë§ (NPB ê³µì‹ ì‚¬ì´íŠ¸)"""
        if not CRAWLING_ENABLED:
            return []
            
        # NPB ê³µì‹ ì‚¬ì´íŠ¸ URL í˜•ì‹ (ì¼ë³¸ì–´)
        # https://npb.jp/bis/2025/calendar/index_09.html (ì›”ë³„)
        year = target_date.year
        month = target_date.month
        day_num = target_date.day
        
        # NPB ì›”ë³„ ìº˜ë¦°ë” í˜ì´ì§€
        url = f"https://npb.jp/bis/{year}/calendar/index_{month:02d}.html"
        
        self.logger.info(f"ğŸ” Checking upcoming games: {target_date.strftime('%Y-%m-%d')}")
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            # Use raw bytes so BeautifulSoup can detect UTF-8 from meta
            soup = BeautifulSoup(response.content, 'html.parser')
            games = []
            
            # NPB ìº˜ë¦°ë” í…Œì´ë¸”ì—ì„œ íŠ¹ì • ë‚ ì§œ ì°¾ê¸°
            calendar_table = soup.find('table', class_='tetblmain')
            if not calendar_table:
                self.logger.warning(f"âš ï¸ Calendar table not found for {target_date.strftime('%Y-%m-%d')}")
                return []
            
            # ëª¨ë“  ë‚ ì§œ ì…€ ì°¾ê¸°
            date_cells = calendar_table.find_all('td', class_='stschedule')
            
            for cell in date_cells:
                # ë‚ ì§œ í™•ì¸
                date_div = cell.find('div', class_='teschedate')
                if not date_div:
                    continue
                    
                # ë‚ ì§œ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ë§í¬ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
                date_text = date_div.get_text(strip=True)
                try:
                    cell_day = int(date_text)
                except ValueError:
                    continue
                
                if cell_day == day_num:
                    self.logger.info(f"ğŸ“… Found date cell for day {day_num}")
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ê²½ê¸° ì •ë³´ ì¶”ì¶œ
                    game_divs = cell.find_all('div', class_='stvsteam')
                    self.logger.info(f"ğŸ“… Found {len(game_divs)} game div containers")
                    
                    for i, game_div in enumerate(game_divs):
                        game_texts = game_div.find_all('div')
                        self.logger.info(f"ğŸ“… Game div {i}: found {len(game_texts)} game text divs")
                        
                        for j, game_text_div in enumerate(game_texts):
                            game_text = game_text_div.get_text(strip=True)
                            self.logger.info(f"ğŸ“… Game text {j}: '{game_text}'")
                            
                            # ê²½ê¸° ì‹œê°„ì´ ìˆëŠ” ì˜ˆì • ê²½ê¸°ë§Œ ì²˜ë¦¬ (18:00, 14:00 ë“±)
                            if 'ï¼š' in game_text and ('-' in game_text or 'vs' in game_text):
                                self.logger.info(f"ğŸ“… Processing scheduled game: '{game_text}'")
                                try:
                                    # íŒ€ëª…ê³¼ ì‹œê°„ ë¶„ë¦¬ (ì˜ˆ: "å·¨ - ãƒ¤ã€€18ï¼š00")
                                    parts = game_text.split('ã€€')
                                    if len(parts) >= 2:
                                        team_part = parts[0].strip()
                                        time_part = parts[1].strip()
                                        self.logger.info(f"ğŸ“… Team part: '{team_part}', Time part: '{time_part}'")
                                        
                                        # íŒ€ëª… ì¶”ì¶œ
                                        if '-' in team_part:
                                            team_names = team_part.split('-')
                                        elif 'vs' in team_part:
                                            team_names = team_part.split('vs')
                                        else:
                                            self.logger.warning(f"âš ï¸ No separator found in team part: {team_part}")
                                            continue
                                            
                                        if len(team_names) >= 2:
                                            away_team_text = team_names[0].strip()
                                            home_team_text = team_names[1].strip()
                                            self.logger.info(f"ğŸ“… Away: '{away_team_text}', Home: '{home_team_text}'")
                                            
                                            away_team = self.get_team_info(away_team_text)
                                            home_team = self.get_team_info(home_team_text)
                                            
                                            if away_team and home_team:
                                                # ë¦¬ê·¸ íŒë‹¨
                                                league = away_team['league']
                                                
                                                game = {
                                                    'date': target_date.strftime('%Y-%m-%d'),
                                                    'home_team_id': home_team['id'],
                                                    'home_team_name': home_team['name'],
                                                    'home_team_abbr': home_team['abbr'],
                                                    'away_team_id': away_team['id'],
                                                    'away_team_name': away_team['name'],
                                                    'away_team_abbr': away_team['abbr'],
                                                    'home_score': None,  # ì˜ˆì • ê²½ê¸°ëŠ” ì ìˆ˜ ì—†ìŒ
                                                    'away_score': None,
                                                    'league': league,
                                                    'status': 'scheduled',
                                                    'is_draw': False,
                                                    'winner': None,
                                                    'game_time': time_part
                                                }
                                                
                                                games.append(game)
                                                self.logger.info(f"ğŸ“… Scheduled: {away_team['abbr']} vs {home_team['abbr']} at {time_part}")
                                            else:
                                                self.logger.warning(f"âš ï¸ Team not found: away='{away_team_text}', home='{home_team_text}'")
                                                
                                except Exception as e:
                                    self.logger.warning(f"âš ï¸ Failed to parse game: {game_text} - {e}")
                                    continue
                    
                    break  # í•´ë‹¹ ë‚ ì§œë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë£¨í”„ ì¢…ë£Œ
            
            return games
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to crawl upcoming games for {target_date.strftime('%Y-%m-%d')}: {e}")
            return []

def main():
    import sys
    
    crawler = SimpleCrawler()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == '--full-season':
            # ì „ì²´ ì‹œì¦Œ í¬ë¡¤ë§ (3ì›” 28ì¼ë¶€í„°)
            games_count = crawler.crawl_full_season("2025-03-28")
            print(f"\nğŸ† Full season crawl completed: {games_count} games collected")
        elif sys.argv[1] == '--test':
            games_count = crawler.crawl_multiple_days(3)
            print(f"\nâœ… Test crawl completed: {games_count} games collected")
        elif sys.argv[1] == '--quick':
            games_count = crawler.crawl_multiple_days(1)
            print(f"\nâš¡ Quick crawl completed: {games_count} games collected")
        elif sys.argv[1] == '--upcoming':
            # ì˜ˆì • ê²½ê¸° í¬ë¡¤ë§ (ê¸°ë³¸ 30ì¼)
            upcoming_games = crawler.crawl_upcoming_games(30)
            games_count = len(upcoming_games)
            print(f"\nğŸ“… Upcoming games crawl completed: {games_count} games found")
        else:
            try:
                days = int(sys.argv[1])
                games_count = crawler.crawl_multiple_days(days)
                print(f"\nâœ… Crawl completed: {games_count} games collected")
            except ValueError:
                print("âŒ Invalid argument. Available options:")
                print("  --full-season    : Crawl entire season")
                print("  --test           : Test crawl (3 days)")
                print("  --quick          : Quick crawl (1 day)")
                print("  --upcoming       : Upcoming games (30 days)")
                print("  <number>         : Crawl specific number of days")
                return 1
    else:
        # ê¸°ë³¸: 7ì¼
        games_count = crawler.crawl_multiple_days(7)
        print(f"\nâœ… Default crawl completed: {games_count} games collected")
    
    return 0 if games_count > 0 else 1

if __name__ == "__main__":
    exit(main())
