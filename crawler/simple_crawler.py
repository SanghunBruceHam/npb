#!/usr/bin/env python3
"""
NPB Simple Crawler - PostgreSQL ì—†ì´ ì§ì ‘ TXT ì €ì¥
í¬ë¡¤ë§ â†’ TXT ì €ì¥ â†’ JavaScript ì²˜ë¦¬ â†’ JSON
"""

try:
    import requests
    from bs4 import BeautifulSoup
    CRAWLING_ENABLED = True
except ImportError:
    print("âš ï¸ Web crawling dependencies not available (requests, beautifulsoup4)")
    print("ğŸ“„ Using existing data conversion instead...")
    CRAWLING_ENABLED = False
    requests = None
    BeautifulSoup = None

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import time
import logging
import sys

class SimpleCrawler:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data" / "simple"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.setup_logging()
        
        # NPB íŒ€ ì •ë³´ (ì›¹ì‚¬ì´íŠ¸ í‘œì‹œëª… ê¸°ì¤€)
        self.teams = {
            # ì„¼íŠ¸ëŸ´ë¦¬ê·¸
            'ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„': {'id': 1, 'abbr': 'YOG', 'name': 'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„', 'league': 'Central'},
            'å·¨äºº': {'id': 1, 'abbr': 'YOG', 'name': 'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„', 'league': 'Central'},
            'é˜ªç¥': {'id': 2, 'abbr': 'HAN', 'name': 'é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹', 'league': 'Central'},
            'ï¼¤ï½…ï¼®ï¼¡': {'id': 3, 'abbr': 'YDB', 'name': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'league': 'Central'},
            'DeNA': {'id': 3, 'abbr': 'YDB', 'name': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º', 'league': 'Central'},
            'ä¸­æ—¥': {'id': 5, 'abbr': 'CHU', 'name': 'ä¸­æ—¥ãƒ‰ãƒ©ã‚´ãƒ³ã‚º', 'league': 'Central'},
            'åºƒå³¶': {'id': 4, 'abbr': 'HIR', 'name': 'åºƒå³¶æ±æ´‹ã‚«ãƒ¼ãƒ—', 'league': 'Central'},
            'ãƒ¤ã‚¯ãƒ«ãƒˆ': {'id': 6, 'abbr': 'YAK', 'name': 'æ±äº¬ãƒ¤ã‚¯ãƒ«ãƒˆã‚¹ãƒ¯ãƒ­ãƒ¼ã‚º', 'league': 'Central'},
            
            # í¼ì‹œí”½ë¦¬ê·¸
            'ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯': {'id': 7, 'abbr': 'SOF', 'name': 'ç¦å²¡ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ãƒ›ãƒ¼ã‚¯ã‚¹', 'league': 'Pacific'},
            'ãƒ­ãƒƒãƒ†': {'id': 8, 'abbr': 'LOT', 'name': 'åƒè‘‰ãƒ­ãƒƒãƒ†ãƒãƒªãƒ¼ãƒ³ã‚º', 'league': 'Pacific'},
            'æ¥½å¤©': {'id': 9, 'abbr': 'RAK', 'name': 'æ±åŒ—æ¥½å¤©ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¤ãƒ¼ã‚°ãƒ«ã‚¹', 'league': 'Pacific'},
            'ã‚ªãƒªãƒƒã‚¯ã‚¹': {'id': 10, 'abbr': 'ORI', 'name': 'ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º', 'league': 'Pacific'},
            'è¥¿æ­¦': {'id': 11, 'abbr': 'SEI', 'name': 'åŸ¼ç‰è¥¿æ­¦ãƒ©ã‚¤ã‚ªãƒ³ã‚º', 'league': 'Pacific'},
            'æ—¥æœ¬ãƒãƒ ': {'id': 12, 'abbr': 'NIP', 'name': 'åŒ—æµ·é“æ—¥æœ¬ãƒãƒ ãƒ•ã‚¡ã‚¤ã‚¿ãƒ¼ã‚º', 'league': 'Pacific'}
        }
    
    def setup_logging(self):
        log_dir = self.project_root / "logs" / "simple_crawler"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = log_dir / f"crawler_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('simple_crawler')
    
    def get_team_info(self, team_name):
        """íŒ€ëª…ìœ¼ë¡œ íŒ€ ì •ë³´ ì°¾ê¸°"""
        for key, info in self.teams.items():
            if key in team_name:
                return info
        return None
    
    def convert_db_data_to_txt(self):
        """ê¸°ì¡´ DB ë°ì´í„°ë¥¼ TXTë¡œ ë³€í™˜ (fallback)"""
        self.logger.info("ğŸ“„ Converting existing database data to TXT format...")
        
        try:
            # Use existing db_to_simple_txt script
            db_script = self.project_root / 'scripts' / 'db_to_simple_txt.py'
            if db_script.exists():
                result = os.system(f"cd {self.project_root} && python3 {db_script}")
                if result == 0:
                    self.logger.info("âœ… DB to TXT conversion completed")
                    return True
                else:
                    self.logger.error("âŒ DB to TXT conversion failed")
                    return False
            else:
                self.logger.error("âŒ DB conversion script not found")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ DB conversion error: {e}")
            return False

    def crawl_date(self, target_date):
        """íŠ¹ì • ë‚ ì§œì˜ ê²½ê¸° ê²°ê³¼ í¬ë¡¤ë§"""
        if not CRAWLING_ENABLED:
            return []  # Skip actual crawling if dependencies unavailable
            
        # URL í˜•ì‹: https://www.nikkansports.com/baseball/professional/score/2025/pf-score-20250328.html
        date_str = target_date.strftime("%Y%m%d")
        year = target_date.strftime("%Y")
        url = f"https://www.nikkansports.com/baseball/professional/score/{year}/pf-score-{date_str}.html"
        
        self.logger.info(f"ğŸ” Crawling: {target_date.strftime('%Y-%m-%d')}")
        
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            games = []
            
            # scoreTable í´ë˜ìŠ¤ì˜ í…Œì´ë¸”ë“¤ì—ì„œ ê²½ê¸° ê²°ê³¼ íŒŒì‹±
            score_tables = soup.find_all('table', class_='scoreTable')
            
            for table in score_tables:
                try:
                    rows = table.find_all('tr')
                    if len(rows) < 3:  # í—¤ë” + 2íŒ€ ìµœì†Œ í•„ìš”
                        continue
                    
                    # íŒ€ëª… ì¶”ì¶œ (ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ í–‰)
                    away_row = rows[1]  # ì²« ë²ˆì§¸ íŒ€ (away)
                    home_row = rows[2]  # ë‘ ë²ˆì§¸ íŒ€ (home)
                    
                    # íŒ€ëª…ì—ì„œ ê³µë°± ì œê±°í•˜ê³  ë§¤í•‘
                    away_team_text = away_row.find('td', class_='team').get_text(strip=True).replace('\xa0', '')
                    home_team_text = home_row.find('td', class_='team').get_text(strip=True).replace('\xa0', '')
                    
                    away_team = self.get_team_info(away_team_text)
                    home_team = self.get_team_info(home_team_text)
                    
                    if not away_team or not home_team:
                        self.logger.warning(f"âš ï¸ Team not found: {away_team_text} vs {home_team_text}")
                        continue
                    
                    # totalScore í´ë˜ìŠ¤ì—ì„œ ì´ì  ì¶”ì¶œ
                    away_score_cell = away_row.find('td', class_='totalScore')
                    home_score_cell = home_row.find('td', class_='totalScore')
                    
                    if not away_score_cell or not home_score_cell:
                        self.logger.warning(f"âš ï¸ Could not find totalScore cells")
                        continue
                    
                    # ìˆ«ìë§Œ ì¶”ì¶œ (í•œì ìˆ«ìë„ ì²˜ë¦¬)
                    away_score_text = away_score_cell.get_text(strip=True)
                    home_score_text = home_score_cell.get_text(strip=True)
                    
                    # í•œì ìˆ«ìë¥¼ ì•„ë¼ë¹„ì•„ ìˆ«ìë¡œ ë³€í™˜
                    def convert_jp_number(text):
                        jp_to_num = {'ï¼': 0, 'ï¼‘': 1, 'ï¼’': 2, 'ï¼“': 3, 'ï¼”': 4, 'ï¼•': 5, 'ï¼–': 6, 'ï¼—': 7, 'ï¼˜': 8, 'ï¼™': 9}
                        if text in jp_to_num:
                            return jp_to_num[text]
                        try:
                            return int(text)
                        except:
                            return 0
                    
                    away_score = convert_jp_number(away_score_text)
                    home_score = convert_jp_number(home_score_text)
                    
                    # ë¦¬ê·¸ íŒë‹¨ (íŒ€ ì •ë³´ì—ì„œ)
                    league = away_team['league']
                    
                    # 0-0 ê²½ê¸°ëŠ” NPBì—ì„œ ì¬ê²½ê¸°ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì œì™¸
                    if home_score == 0 and away_score == 0:
                        self.logger.info(f"â­ï¸  Skipping 0-0 game: {away_team['abbr']} vs {home_team['abbr']} (postponed/rescheduled)")
                        continue
                    
                    # ê²½ê¸° ì •ë³´
                    game = {
                        'date': target_date.strftime('%Y-%m-%d'),
                        'home_team_id': home_team['id'],
                        'home_team_name': home_team['name'],
                        'home_team_abbr': home_team['abbr'],
                        'away_team_id': away_team['id'],
                        'away_team_name': away_team['name'],
                        'away_team_abbr': away_team['abbr'],
                        'home_score': home_score,
                        'away_score': away_score,
                        'league': league,
                        'status': 'completed',
                        'is_draw': home_score == away_score,  # ì‹¤ì œ ë™ì ë§Œ ë¬´ìŠ¹ë¶€
                        'winner': 'home' if home_score > away_score else ('away' if away_score > home_score else 'draw')
                    }
                    
                    games.append(game)
                    self.logger.info(f"âœ… Parsed: {away_team['abbr']} {away_score}-{home_score} {home_team['abbr']}")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Failed to parse table: {e}")
                    continue
            
            self.logger.info(f"âœ… Found {len(games)} games on {target_date.strftime('%Y-%m-%d')}")
            return games
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to crawl {target_date.strftime('%Y-%m-%d')}: {e}")
            return []
    
    def save_games_to_txt(self, games, filename="games_raw.txt"):
        """ê²½ê¸° ê²°ê³¼ë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥"""
        if not games:
            return
        
        file_path = self.data_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ ì½ê¸° (ì¤‘ë³µ ë°©ì§€)
        existing_games = []
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith('#') or not line.strip():
                            continue
                        existing_games.append(line.strip())
            except Exception as e:
                self.logger.warning(f"Failed to read existing file: {e}")
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€
        new_lines = []
        existing_set = set(existing_games)
        
        for game in games:
            # TXT í˜•ì‹: DATE|HOME_ID|HOME_ABBR|HOME_NAME|AWAY_ID|AWAY_ABBR|AWAY_NAME|HOME_SCORE|AWAY_SCORE|LEAGUE|STATUS|IS_DRAW
            line = "|".join([
                game['date'],
                str(game['home_team_id']),
                game['home_team_abbr'], 
                game['home_team_name'],
                str(game['away_team_id']),
                game['away_team_abbr'],
                game['away_team_name'],
                str(game['home_score']),
                str(game['away_score']),
                game['league'],
                game['status'],
                '1' if game['is_draw'] else '0'
            ])
            
            if line not in existing_set:
                new_lines.append(line)
                existing_set.add(line)
        
        if new_lines:
            # íŒŒì¼ì— ì¶”ê°€
            with open(file_path, 'a', encoding='utf-8') as f:
                if file_path.stat().st_size == 0:
                    # ìƒˆ íŒŒì¼ì¸ ê²½ìš° í—¤ë” ì¶”ê°€
                    f.write("# NPB_GAMES_DATA\n")
                    f.write(f"# UPDATED: {datetime.now().isoformat()}\n") 
                    f.write("# FORMAT: DATE|HOME_ID|HOME_ABBR|HOME_NAME|AWAY_ID|AWAY_ABBR|AWAY_NAME|HOME_SCORE|AWAY_SCORE|LEAGUE|STATUS|IS_DRAW\n")
                
                for line in new_lines:
                    f.write(line + '\n')
            
            self.logger.info(f"ğŸ“„ Saved {len(new_lines)} new games to {file_path}")
        else:
            self.logger.info("ğŸ“„ No new games to save")
    
    def save_teams_to_txt(self):
        """íŒ€ ì •ë³´ë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥"""
        file_path = self.data_dir / "teams_raw.txt"
        
        lines = []
        lines.append("# NPB_TEAMS_DATA")
        lines.append(f"# UPDATED: {datetime.now().isoformat()}")
        lines.append("# FORMAT: TEAM_ID|TEAM_ABBR|TEAM_NAME|LEAGUE")
        
        for team_info in self.teams.values():
            line = "|".join([
                str(team_info['id']),
                team_info['abbr'],
                team_info['name'],
                team_info['league']
            ])
            lines.append(line)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        self.logger.info(f"ğŸ“„ Saved teams to {file_path}")
    
    def crawl_full_season(self, start_date="2025-03-28"):
        """NPB ì‹œì¦Œ ì „ì²´ í¬ë¡¤ë§ (3ì›” 28ì¼ë¶€í„°)"""
        self.logger.info(f"ğŸš€ Starting full NPB season crawl from {start_date}...")
        
        if not CRAWLING_ENABLED:
            self.logger.info("ğŸ“„ Web crawling not available, using existing data...")
            # Use existing database data instead
            if self.convert_db_data_to_txt():
                self.save_teams_to_txt()
                return 1  # Success
            else:
                return 0
        
        all_games = []
        start = datetime.strptime(start_date, "%Y-%m-%d")
        today = datetime.now()
        
        current_date = start
        total_days = (today - start).days + 1
        
        self.logger.info(f"ğŸ“… Crawling {total_days} days from {start_date} to {today.strftime('%Y-%m-%d')}")
        
        day_count = 0
        while current_date <= today:
            day_count += 1
            games = self.crawl_date(current_date)
            
            if games:
                all_games.extend(games)
                self.logger.info(f"ğŸ“… {current_date.strftime('%Y-%m-%d')}: {len(games)} games")
            else:
                # ê²½ê¸° ì—†ëŠ” ë‚  (íœ´ì‹ì¼)
                pass
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if day_count % 10 == 0 or day_count == total_days:
                progress = (day_count / total_days) * 100
                self.logger.info(f"ğŸ”„ Progress: {day_count}/{total_days} days ({progress:.1f}%)")
            
            current_date += timedelta(days=1)
            
            # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)
        
        # ê²½ê¸° ê²°ê³¼ ì €ì¥
        if all_games:
            self.save_games_to_txt(all_games)
        
        # íŒ€ ì •ë³´ ì €ì¥
        self.save_teams_to_txt()
        
        self.logger.info(f"ğŸ† **FULL SEASON CRAWL SUMMARY**")
        self.logger.info(f"Total games: {len(all_games)}")
        self.logger.info(f"Draws: {sum(1 for g in all_games if g['is_draw'])}")
        self.logger.info(f"Period: {start_date} to {today.strftime('%Y-%m-%d')}")
        
        # ì‹œì¦Œ í†µê³„
        if all_games:
            teams_count = {}
            for game in all_games:
                home_team = game['home_team_abbr']
                away_team = game['away_team_abbr']
                teams_count[home_team] = teams_count.get(home_team, 0) + 1
                teams_count[away_team] = teams_count.get(away_team, 0) + 1
            
            self.logger.info("ğŸ“Š **TEAM GAMES COUNT**:")
            for team, count in sorted(teams_count.items()):
                self.logger.info(f"  {team}: {count} games")
        
        return len(all_games)

    def crawl_multiple_days(self, days=7):
        """ì—¬ëŸ¬ ë‚ ì§œ í¬ë¡¤ë§ (ë˜ëŠ” ê¸°ì¡´ ë°ì´í„° ë³€í™˜)"""
        self.logger.info(f"ğŸš€ Starting simple crawl for last {days} days...")
        
        if not CRAWLING_ENABLED:
            self.logger.info("ğŸ“„ Web crawling not available, using existing data...")
            # Use existing database data instead
            if self.convert_db_data_to_txt():
                self.save_teams_to_txt()
                return 1  # Success
            else:
                return 0
        
        all_games = []
        today = datetime.now()
        
        for i in range(1, days + 1):  # ì–´ì œë¶€í„° ì‹œì‘
            target_date = today - timedelta(days=i)
            games = self.crawl_date(target_date)
            all_games.extend(games)
            
            # ìš”ì²­ ê°„ê²©
            if i < days:
                time.sleep(1)
        
        # ê²½ê¸° ê²°ê³¼ ì €ì¥
        if all_games:
            self.save_games_to_txt(all_games)
        
        # íŒ€ ì •ë³´ ì €ì¥
        self.save_teams_to_txt()
        
        self.logger.info(f"ğŸ† **SIMPLE CRAWL SUMMARY**")
        self.logger.info(f"Total games: {len(all_games)}")
        self.logger.info(f"Draws: {sum(1 for g in all_games if g['is_draw'])}")
        
        return len(all_games)

def main():
    import sys
    
    crawler = SimpleCrawler()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == '--full-season':
            # ì „ì²´ ì‹œì¦Œ í¬ë¡¤ë§ (3ì›” 28ì¼ë¶€í„°)
            games_count = crawler.crawl_full_season("2025-03-28")
            print(f"\nğŸ† Full season crawl completed: {games_count} games collected")
        elif sys.argv[1] == '--test':
            games_count = crawler.crawl_multiple_days(3)
            print(f"\nâœ… Test crawl completed: {games_count} games collected")
        elif sys.argv[1] == '--quick':
            games_count = crawler.crawl_multiple_days(1)
            print(f"\nâš¡ Quick crawl completed: {games_count} games collected")
        else:
            try:
                days = int(sys.argv[1])
                games_count = crawler.crawl_multiple_days(days)
                print(f"\nâœ… Crawl completed: {games_count} games collected")
            except ValueError:
                print("âŒ Invalid argument. Use: days, --full-season, --test, or --quick")
                return 1
    else:
        # ê¸°ë³¸: 7ì¼
        games_count = crawler.crawl_multiple_days(7)
        print(f"\nâœ… Default crawl completed: {games_count} games collected")
    
    return 0 if games_count > 0 else 1

if __name__ == "__main__":
    exit(main())